from openai import AsyncOpenAI
import asyncio
import json
import os
from enum import Enum
from typing import Any, List, Dict, Optional
from pydantic import BaseModel, Field
import streamlit as st
import re

# ê¸°ë³¸ ëª¨ë¸ ì •ì˜
class Role(str, Enum):
    """Role enum for chat messages"""
    user = "user"
    assistant = "assistant"
    system = "system"

class ChatMessage(BaseModel):
    """Single chat message used in few-shot examples"""
    role: Role
    content: str

class Issues(BaseModel):
    """Structured output returned by checkers."""
    has_issues: bool
    issues: List[str]
    severity: str = "medium"  # low, medium, high
    category: str = "general"  # general, clarity, specificity, instruction_following, etc.

    @classmethod
    def no_issues(cls) -> "Issues":
        return cls(has_issues=False, issues=[], severity="low", category="general")

class PromptAnalysis(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼"""
    clarity_score: float  # 0-10
    specificity_score: float  # 0-10
    instruction_following_score: float  # 0-10
    overall_score: float  # 0-10
    recommendations: List[str]
    strengths: List[str]
    weaknesses: List[str]

class OptimizedPrompt(BaseModel):
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ê²°ê³¼"""
    original_prompt: str
    optimized_prompt: str
    changes_made: List[str]
    improvement_explanation: str
    estimated_improvement: float  # percentage

class UserFeedback(BaseModel):
    """ì‚¬ìš©ì í”¼ë“œë°±"""
    feedback_text: str
    feedback_type: str = "general"  # general, specific_issue, improvement_request
    priority: str = "medium"  # low, medium, high
    category: str = "general"  # clarity, specificity, instruction_following, agentic_capabilities, format, other

class FeedbackAnalysis(BaseModel):
    """í”¼ë“œë°± ë¶„ì„ ê²°ê³¼"""
    understood_feedback: str
    feedback_category: str
    required_changes: List[str]
    revision_strategy: str
    estimated_impact: float  # 0-10

class RevisedPrompt(BaseModel):
    """í”¼ë“œë°± ê¸°ë°˜ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸"""
    original_optimized_prompt: str
    user_feedback: str
    revised_prompt: str
    changes_made: List[str]
    feedback_addressed: List[str]
    improvement_explanation: str

class GeneralResponse(BaseModel):
    """ë²”ìš© Agent ì‘ë‹µ"""
    original_prompt: str
    modified_prompt: str
    changes_made: List[str]
    explanation: str
    feedback_addressed: str

# ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€
class PromptTypeDetection(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ìœ í˜• ê°ì§€ ê²°ê³¼"""
    detected_type: str  # creative_writing, code_generation, qa, analysis, instruction_following, etc.
    confidence: float  # 0-1
    type_characteristics: List[str]
    optimization_strategy: str
    relevant_checkers: List[str]  # í•´ë‹¹ ìœ í˜•ì— ì í•©í•œ ì²´ì»¤ë“¤

class PromptCandidateOutput(BaseModel):
    """í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„± ê²°ê³¼"""
    prompt_candidates: List[str]

class PromptEvaluationScores(BaseModel):
    """ê°œë³„ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì ìˆ˜"""
    prompt: str
    format_score: float
    contradiction_score: float
    relevance_score: float

class PerformanceRankingOutput(BaseModel):
    """ì„±ëŠ¥ ìˆœìœ„ ê²°ê³¼"""
    ranked_prompts: List[Dict[str, Any]]  # [{"prompt": str, "final_score": float, "rank": int}]

class RelevanceEvaluationOutput(BaseModel):
    """ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼"""
    alignment_score: float  # 0.0 to 1.0
    evaluation_summary: str

class SafetyCheckOutput(BaseModel):
    """ì•ˆì „ì„± ê²€ì‚¬ ê²°ê³¼"""
    is_safe: bool
    safety_flags: List[Dict[str, str]]  # [{"category": str, "details": str}]

# Agent êµ¬í˜„
class Agent:
    def __init__(self, name: str, model: str, output_type: type, instructions: str):
        self.name = name
        self.model = model
        self.output_type = output_type
        self.instructions = instructions

class Runner:
    @staticmethod
    def _get_json_schema(model_class):
        """Pydantic ëª¨ë¸ì˜ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        if model_class == Issues:
            return '''{
  "has_issues": boolean,
  "issues": ["string"],
  "severity": "low|medium|high",
  "category": "string"
}'''
        elif model_class == OptimizedPrompt:
            return '''{
  "original_prompt": "string",
  "optimized_prompt": "string", 
  "changes_made": ["string"],
  "improvement_explanation": "string",
  "estimated_improvement": number
}'''
        elif model_class == FeedbackAnalysis:
            return '''{
  "understood_feedback": "string",
  "feedback_category": "string",
  "required_changes": ["string"],
  "revision_strategy": "string",
  "estimated_impact": number
}'''
        elif model_class == RevisedPrompt:
            return '''{
  "original_optimized_prompt": "string",
  "user_feedback": "string",
  "revised_prompt": "string",
  "changes_made": ["string"],
  "feedback_addressed": ["string"],
  "improvement_explanation": "string"
}'''
        elif model_class == GeneralResponse:
            return '''{
  "original_prompt": "string",
  "modified_prompt": "string",
  "changes_made": ["string"],
  "explanation": "string",
  "feedback_addressed": "string"
}'''
        elif model_class == PromptTypeDetection:
            return '''{
  "detected_type": "string",
  "confidence": number,
  "type_characteristics": ["string"],
  "optimization_strategy": "string",
  "relevant_checkers": ["string"]
}'''
        elif model_class == PromptCandidateOutput:
            return '''{
  "prompt_candidates": ["string"]
}'''
        elif model_class == PerformanceRankingOutput:
            return '''{
  "ranked_prompts": [
    {"prompt": "string", "final_score": number, "rank": number}
  ]
}'''
        elif model_class == RelevanceEvaluationOutput:
            return '''{
  "alignment_score": number,
  "evaluation_summary": "string"
}'''
        elif model_class == SafetyCheckOutput:
            return '''{
  "is_safe": boolean,
  "safety_flags": [
    {"category": "string", "details": "string"}
  ]
}'''
        else:
            return "{}"
    
    @staticmethod
    async def run(agent: Agent, input_data: str, progress_callback=None, api_key: str = None):
        if progress_callback:
            progress_callback(f"ğŸ” Agent '{agent.name}' ì‹¤í–‰ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
        if not api_key:
            return await Runner._run_simulation(agent, input_data, progress_callback)
        
        # ì‹¤ì œ OpenAI API í˜¸ì¶œ
        try:
            client = AsyncOpenAI(api_key=api_key)
            
            # Agentë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = f"""You are an expert prompt optimizer specializing in {agent.name.replace('_', ' ')}.
            
            {agent.instructions}
            
            Analyze the following input and provide a structured response in JSON format.
            
            IMPORTANT: Return ONLY the JSON object that matches this exact structure:
            {Runner._get_json_schema(agent.output_type)}
            
            Do not include any additional text, explanations, or markdown formatting.
            """
            
            response = await client.chat.completions.create(
                model=agent.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": input_data}
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            # JSON ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦
            response_content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result_data = json.loads(response_content)
            except json.JSONDecodeError as e:
                if progress_callback:
                    progress_callback(f"âŒ {agent.name} JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
                return await Runner._run_simulation(agent, input_data, progress_callback)
            
            # Pydantic ëª¨ë¸ ê²€ì¦ ë° ë³€í™˜
            try:
                result = agent.output_type(**result_data)
            except Exception as e:
                if progress_callback:
                    progress_callback(f"âŒ {agent.name} ëª¨ë¸ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
                # ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
                return await Runner._run_simulation(agent, input_data, progress_callback)
            
            if progress_callback:
                progress_callback(f"âœ… {agent.name} ì™„ë£Œ")
            
            return Result(result)
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ {agent.name} ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
            return await Runner._run_simulation(agent, input_data, progress_callback)
    
    @staticmethod
    async def _run_simulation(agent: Agent, input_data: str, progress_callback=None):
        """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ - API í‚¤ê°€ ì—†ì„ ë•Œ ì‚¬ìš©"""
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        # GPT-4.1 ê°€ì´ë“œ ê¸°ë°˜ ë¶„ì„ ë¡œì§
        if agent.name == "clarity_checker":
            return await Runner._analyze_clarity(agent, input_data, progress_callback)
        elif agent.name == "specificity_checker":
            return await Runner._analyze_specificity(agent, input_data, progress_callback)
        elif agent.name == "instruction_following_checker":
            return await Runner._analyze_instruction_following(agent, input_data, progress_callback)
        elif agent.name == "agentic_capability_checker":
            return await Runner._analyze_agentic_capabilities(agent, input_data, progress_callback)
        elif agent.name == "prompt_optimizer":
            return await Runner._optimize_prompt(agent, input_data, progress_callback)
        elif agent.name == "few_shot_optimizer":
            return await Runner._optimize_few_shot(agent, input_data, progress_callback)
        elif agent.name == "feedback_analyzer":
            return await Runner._analyze_feedback(agent, input_data, progress_callback)
        elif agent.name == "prompt_reviser":
            return await Runner._revise_prompt_with_feedback(agent, input_data, progress_callback)
        elif agent.name == "general_agent":
            return await Runner._general_response(agent, input_data, progress_callback)
        
        return Result(agent.output_type.no_issues() if hasattr(agent.output_type, 'no_issues') else {})

    @staticmethod
    async def _analyze_clarity(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ“‹ ëª…í™•ì„± ë¶„ì„ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        issues = []
        
        # ëª…í™•ì„± ì²´í¬ ë¡œì§ (GPT-4.1 ê°€ì´ë“œ ê¸°ë°˜)
        if len(input_data.strip()) < 20:
            issues.append("í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ëª…í™•í•œ ì§€ì‹œì‚¬í•­ì„ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤")
        
        if not any(keyword in input_data.lower() for keyword in ['you are', 'task', 'goal', 'objective']):
            issues.append("ì—­í• ì´ë‚˜ ëª©í‘œê°€ ëª…í™•í•˜ê²Œ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        if input_data.count('?') > 5:
            issues.append("ë„ˆë¬´ ë§ì€ ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ í˜¼ë€ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        ambiguous_words = ['maybe', 'perhaps', 'might', 'could be', 'possibly']
        if any(word in input_data.lower() for word in ambiguous_words):
            issues.append("ëª¨í˜¸í•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆì–´ ëª…í™•ì„±ì„ í•´ì¹©ë‹ˆë‹¤")
        
        result = agent.output_type(
            has_issues=len(issues) > 0,
            issues=issues,
            severity="high" if len(issues) > 2 else "medium" if len(issues) > 0 else "low",
            category="clarity"
        )
        
        if progress_callback:
            progress_callback(f"âœ… ëª…í™•ì„± ë¶„ì„ ì™„ë£Œ: {len(issues)}ê°œ ë¬¸ì œ ë°œê²¬")
        
        return Result(result)

    @staticmethod
    async def _analyze_specificity(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ¯ êµ¬ì²´ì„± ë¶„ì„ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        issues = []
        
        # êµ¬ì²´ì„± ì²´í¬ ë¡œì§
        vague_instructions = ['do something', 'help me', 'make it better', 'improve']
        if any(instruction in input_data.lower() for instruction in vague_instructions):
            issues.append("ì§€ì‹œì‚¬í•­ì´ ë„ˆë¬´ ì¶”ìƒì ì…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ í–‰ë™ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”")
        
        if not any(keyword in input_data.lower() for keyword in ['format', 'structure', 'example', 'template']):
            issues.append("ì¶œë ¥ í˜•ì‹ì´ë‚˜ êµ¬ì¡°ì— ëŒ€í•œ ëª…ì‹œì  ì§€ì¹¨ì´ ì—†ìŠµë‹ˆë‹¤")
        
        if len(input_data.split()) < 50:
            issues.append("í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤")
        
        # GPT-4.1 ê°€ì´ë“œ: ë„êµ¬ ì‚¬ìš© ë° ê³„íš ìœ ë„ ì²´í¬
        if 'tool' in input_data.lower() and 'plan' not in input_data.lower():
            issues.append("ë„êµ¬ ì‚¬ìš©ì´ ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ ê³„íš ìˆ˜ë¦½ì— ëŒ€í•œ ì§€ì¹¨ì´ ì—†ìŠµë‹ˆë‹¤")
        
        result = agent.output_type(
            has_issues=len(issues) > 0,
            issues=issues,
            severity="medium" if len(issues) > 1 else "low",
            category="specificity"
        )
        
        if progress_callback:
            progress_callback(f"âœ… êµ¬ì²´ì„± ë¶„ì„ ì™„ë£Œ: {len(issues)}ê°œ ë¬¸ì œ ë°œê²¬")
        
        return Result(result)

    @staticmethod
    async def _analyze_instruction_following(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ“ ì§€ì‹œì‚¬í•­ ì¤€ìˆ˜ ë¶„ì„ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        issues = []
        
        # GPT-4.1ì€ ì§€ì‹œì‚¬í•­ì„ ë” ë¬¸ì ê·¸ëŒ€ë¡œ ë”°ë¥´ë¯€ë¡œ ëª…í™•í•œ ì§€ì‹œê°€ ì¤‘ìš”
        if not input_data.strip().endswith('.') and not input_data.strip().endswith('!'):
            issues.append("ì§€ì‹œì‚¬í•­ì´ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ì§€ ì•Šì•„ ëª¨í˜¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        contradictory_words = [('always', 'never'), ('must', 'optional'), ('required', 'if needed')]
        for word1, word2 in contradictory_words:
            if word1 in input_data.lower() and word2 in input_data.lower():
                issues.append(f"'{word1}'ê³¼ '{word2}'ì™€ ê°™ì€ ìƒì¶©ë˜ëŠ” ì§€ì‹œì‚¬í•­ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        
        # ìš°ì„ ìˆœìœ„ ì²´í¬
        if 'important' in input_data.lower() and 'priority' not in input_data.lower():
            issues.append("ì¤‘ìš”ë„ëŠ” ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ ìš°ì„ ìˆœìœ„ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        result = agent.output_type(
            has_issues=len(issues) > 0,
            issues=issues,
            severity="high" if len(issues) > 2 else "medium",
            category="instruction_following"
        )
        
        if progress_callback:
            progress_callback(f"âœ… ì§€ì‹œì‚¬í•­ ì¤€ìˆ˜ ë¶„ì„ ì™„ë£Œ: {len(issues)}ê°œ ë¬¸ì œ ë°œê²¬")
        
        return Result(result)

    @staticmethod
    async def _analyze_agentic_capabilities(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ¤– ì—ì´ì „í‹± ëŠ¥ë ¥ ë¶„ì„ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        issues = []
        
        # GPT-4.1 ê°€ì´ë“œì˜ 3ê°€ì§€ í•µì‹¬ ìš”ì†Œ ì²´í¬
        has_persistence = any(keyword in input_data.lower() for keyword in 
                            ['keep going', 'continue', 'persist', 'until complete', 'multi-step'])
        has_tool_guidance = any(keyword in input_data.lower() for keyword in 
                              ['tools', 'function', 'use available', 'do not guess'])
        has_planning = any(keyword in input_data.lower() for keyword in 
                         ['plan', 'step by step', 'think through', 'reflect'])
        
        if not has_persistence:
            issues.append("ì§€ì†ì„±(persistence) ì§€ì¹¨ì´ ì—†ìŠµë‹ˆë‹¤. ë©€í‹°í„´ ì‘ì—…ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤")
        
        if not has_tool_guidance and 'tool' in input_data.lower():
            issues.append("ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ì´ ì—†ìŠµë‹ˆë‹¤")
        
        if not has_planning:
            issues.append("ê³„íš ìˆ˜ë¦½ ë° ë°˜ì„±ì  ì‚¬ê³ ì— ëŒ€í•œ ì§€ì¹¨ì´ ì—†ìŠµë‹ˆë‹¤")
        
        result = agent.output_type(
            has_issues=len(issues) > 0,
            issues=issues,
            severity="medium",
            category="agentic_capabilities"
        )
        
        if progress_callback:
            progress_callback(f"âœ… ì—ì´ì „í‹± ëŠ¥ë ¥ ë¶„ì„ ì™„ë£Œ: {len(issues)}ê°œ ë¬¸ì œ ë°œê²¬")
        
        return Result(result)

    @staticmethod
    async def _optimize_prompt(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("âœï¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        try:
            data = json.loads(input_data)
            original_prompt = data.get("original_prompt", "")
            all_issues = data.get("all_issues", [])
            
            # GPT-4.1 ê°€ì´ë“œ ê¸°ë°˜ ìµœì í™”
            optimized_prompt = original_prompt
            changes_made = []
            
            # 1. ì—­í•  ëª…í™•í™”
            if not optimized_prompt.lower().startswith('you are'):
                optimized_prompt = "You are a helpful AI assistant. " + optimized_prompt
                changes_made.append("ëª…í™•í•œ ì—­í•  ì •ì˜ ì¶”ê°€")
            
            # 2. GPT-4.1 ì—ì´ì „í‹± êµ¬ì„± ìš”ì†Œ ì¶”ê°€
            agentic_components = []
            
            # Persistence ì¶”ê°€
            if not any(keyword in optimized_prompt.lower() for keyword in ['keep going', 'until complete']):
                agentic_components.append(
                    "Please keep going until the task is completely resolved, before ending your turn."
                )
                changes_made.append("ì§€ì†ì„±(persistence) ì§€ì¹¨ ì¶”ê°€")
            
            # Tool-calling guidance ì¶”ê°€ (í•„ìš”ì‹œ)
            if 'tool' in optimized_prompt.lower() and 'do not guess' not in optimized_prompt.lower():
                agentic_components.append(
                    "If you are not sure about information needed for the task, use available tools to gather relevant information - do NOT guess or make up an answer."
                )
                changes_made.append("ë„êµ¬ ì‚¬ìš© ì§€ì¹¨ ì¶”ê°€")
            
            # Planning guidance ì¶”ê°€
            if not any(keyword in optimized_prompt.lower() for keyword in ['plan', 'step by step']):
                agentic_components.append(
                    "Plan extensively before taking action, and reflect on the outcomes of your actions."
                )
                changes_made.append("ê³„íš ìˆ˜ë¦½ ì§€ì¹¨ ì¶”ê°€")
            
            # 3. êµ¬ì²´ì  ê°œì„ ì‚¬í•­ ì ìš©
            for issue_set in all_issues:
                for issue in issue_set.get('issues', []):
                    if 'ë„ˆë¬´ ì§§' in issue:
                        optimized_prompt += "\n\nPlease provide detailed, comprehensive responses with clear explanations."
                        changes_made.append("ìƒì„¸í•œ ì‘ë‹µ ìš”êµ¬ì‚¬í•­ ì¶”ê°€")
                    elif 'ëª¨í˜¸í•œ í‘œí˜„' in issue:
                        optimized_prompt = optimized_prompt.replace('maybe', 'specifically')
                        optimized_prompt = optimized_prompt.replace('perhaps', 'exactly')
                        changes_made.append("ëª¨í˜¸í•œ í‘œí˜„ ì œê±°")

            # ë§ˆí¬ë‹¤ìš´ ê°•ì¡° ë³µì› (í° ì˜¤ë¥˜ê°€ ì—†ì„ ë•Œë§Œ)
            all_issues_flat = [i for issue_set in all_issues for i in issue_set.get('issues', [])]
            optimized_prompt = preserve_markdown_emphasis(original_prompt, optimized_prompt, all_issues_flat)

            # ì—ì´ì „í‹± êµ¬ì„± ìš”ì†Œ ì¶”ê°€
            if agentic_components:
                optimized_prompt += "\n\n" + "\n".join(agentic_components)

            # 4. ì¶œë ¥ í˜•ì‹ ëª…ì‹œ
            if 'format' not in optimized_prompt.lower():
                optimized_prompt += "\n\nProvide your response in a clear, structured format."
                changes_made.append("ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ ì¶”ê°€")

            # ì¤„ë°”ê¿ˆ ë³´ì¡´: \nì´ ì—†ìœ¼ë©´ ë¬¸ì¥ ëë§ˆë‹¤ ê°•ì œë¡œ ì¶”ê°€
            if '\n' not in optimized_prompt:
                optimized_prompt = re.sub(r'([.!?]) +', r'\1\n', optimized_prompt)

            result = OptimizedPrompt(
                original_prompt=original_prompt,
                optimized_prompt=optimized_prompt,
                changes_made=changes_made,
                improvement_explanation="GPT-4.1 ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ëª…í™•ì„±, êµ¬ì²´ì„±, ì—ì´ì „í‹± ëŠ¥ë ¥ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.",
                estimated_improvement=min(len(changes_made) * 15, 80)  # ìµœëŒ€ 80% ê°œì„ 
            )
            
            if progress_callback:
                progress_callback(f"âœ… í”„ë¡¬í”„íŠ¸ ìµœì í™” ì™„ë£Œ: {len(changes_made)}ê°œ ê°œì„ ì‚¬í•­ ì ìš©")
            
            return Result(result)
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return Result(OptimizedPrompt(
                original_prompt=input_data,
                optimized_prompt=input_data,
                changes_made=[],
                improvement_explanation="ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                estimated_improvement=0
            ))

    @staticmethod
    async def _optimize_few_shot(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ“ Few-shot ì˜ˆì œ ìµœì í™” ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        try:
            data = json.loads(input_data)
            messages = data.get("messages", [])
            optimized_prompt = data.get("optimized_prompt", "")
            
            # Few-shot ì˜ˆì œ ê°œì„ 
            improved_messages = []
            
            for msg in messages:
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    # ë” êµ¬ì²´ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µìœ¼ë¡œ ê°œì„ 
                    if len(content) < 50:
                        content = f"Based on your request, here's a detailed response: {content}. Let me know if you need further clarification or have additional questions."
                    improved_messages.append({"role": "assistant", "content": content})
                else:
                    improved_messages.append(msg)
            
            if progress_callback:
                progress_callback(f"âœ… Few-shot ìµœì í™” ì™„ë£Œ: {len(improved_messages)}ê°œ ë©”ì‹œì§€")
            
            return Result({"messages": improved_messages})
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ Few-shot ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return Result({"messages": []})

    @staticmethod
    async def _analyze_feedback(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("ğŸ“ í”¼ë“œë°± ë¶„ì„ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        try:
            data = json.loads(input_data)
            user_feedback = data.get("user_feedback", "")
            
            # í”¼ë“œë°± ë¶„ì„ ë¡œì§
            understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤."
            feedback_category = "general"
            required_changes = []
            revision_strategy = "ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ ì§€í•˜ê³  í”¼ë“œë°±ì— ë”°ë¼ ê°œì„ í•©ë‹ˆë‹¤."
            estimated_impact = 0.0 # 0-10 ì ìˆ˜

            # ê°„ë‹¨í•œ ë¶„ì„ ë¡œì§ (ì‹¤ì œ ë¶„ì„ì€ ë” ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŒ)
            if "ëª¨í˜¸í•œ í‘œí˜„" in user_feedback:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ëª¨í˜¸í•œ í‘œí˜„ì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤."
                required_changes.append("ëª¨í˜¸í•œ í‘œí˜„ ì œê±°")
                revision_strategy = "ëª¨í˜¸í•œ í‘œí˜„ì„ ì œê±°í•˜ì—¬ ëª…í™•ì„±ì„ ë†’ì´ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.8 # ë†’ì€ ì˜í–¥
            elif "ë„ˆë¬´ ì§§ì€ ì‘ë‹µ" in user_feedback:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì‘ë‹µì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
                required_changes.append("ë” êµ¬ì²´ì ì¸ ì‘ë‹µ ì œê³µ")
                revision_strategy = "ë” êµ¬ì²´ì ì¸ ì‘ë‹µì„ ì œê³µí•˜ì—¬ ëª…í™•ì„±ì„ ë†’ì´ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.6 # ë†’ì€ ì˜í–¥
            elif "ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­" in user_feedback:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•œ ìš°ì„ ìˆœìœ„ë¥¼ ëª…í™•íˆ í•˜ê² ìŠµë‹ˆë‹¤."
                required_changes.append("ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•œ ìš°ì„ ìˆœìœ„ ëª…í™•íˆ í•˜ê¸°")
                revision_strategy = "ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•œ ìš°ì„ ìˆœìœ„ë¥¼ ëª…í™•íˆ í•˜ì—¬ ëª…í™•ì„±ì„ ë†’ì´ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.7 # ë†’ì€ ì˜í–¥
            elif "ë„êµ¬ ì‚¬ìš©" in user_feedback:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ì„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤."
                required_changes.append("ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ ì¶”ê°€")
                revision_strategy = "ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ì„ ì¶”ê°€í•˜ì—¬ ì—ì´ì „í‹± ëŠ¥ë ¥ì„ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.9 # ë†’ì€ ì˜í–¥
            elif "ê³„íš ìˆ˜ë¦½" in user_feedback:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ê³„íš ìˆ˜ë¦½ ë° ë°˜ì„±ì  ì‚¬ê³ ì— ëŒ€í•œ ì§€ì¹¨ì„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤."
                required_changes.append("ê³„íš ìˆ˜ë¦½ ë° ë°˜ì„±ì  ì‚¬ê³ ì— ëŒ€í•œ ì§€ì¹¨ ì¶”ê°€")
                revision_strategy = "ê³„íš ìˆ˜ë¦½ ë° ë°˜ì„±ì  ì‚¬ê³ ì— ëŒ€í•œ ì§€ì¹¨ì„ ì¶”ê°€í•˜ì—¬ ì—ì´ì „í‹± ëŠ¥ë ¥ì„ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.8 # ë†’ì€ ì˜í–¥
            else:
                understood_feedback = "í”¼ë“œë°±ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. í”¼ë“œë°±ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤."
                required_changes = []
                revision_strategy = "í”¼ë“œë°±ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤."
                estimated_impact = 0.5 # ì¤‘ê°„ ì˜í–¥

            result = FeedbackAnalysis(
                understood_feedback=understood_feedback,
                feedback_category=feedback_category,
                required_changes=required_changes,
                revision_strategy=revision_strategy,
                estimated_impact=estimated_impact
            )
            
            if progress_callback:
                progress_callback(f"âœ… í”¼ë“œë°± ë¶„ì„ ì™„ë£Œ: ì´í•´ë„ {estimated_impact:.1f}/10")
            
            return Result(result)
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ í”¼ë“œë°± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return Result(FeedbackAnalysis(
                understood_feedback="í”¼ë“œë°± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                feedback_category="error",
                required_changes=[],
                revision_strategy="í”¼ë“œë°± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                estimated_impact=0
            ))

    @staticmethod
    async def _revise_prompt_with_feedback(agent: Agent, input_data: str, progress_callback=None):
        if progress_callback:
            progress_callback("âœï¸ í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        try:
            data = json.loads(input_data)
            original_optimized_prompt = data.get("original_optimized_prompt", "")
            user_feedback = data.get("user_feedback", "")
            
            # í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ë¡œì§
            revised_prompt = original_optimized_prompt
            changes_made = []
            feedback_addressed = []
            improvement_explanation = "í”¼ë“œë°±ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤."

            # ëª¨í˜¸í•œ í‘œí˜„ ì œê±°
            if "ëª¨í˜¸í•œ í‘œí˜„" in user_feedback:
                revised_prompt = revised_prompt.replace('maybe', 'specifically').replace('perhaps', 'exactly')
                changes_made.append("ëª¨í˜¸í•œ í‘œí˜„ ì œê±°")
                feedback_addressed.append("ëª¨í˜¸í•œ í‘œí˜„ ì œê±°")
                improvement_explanation += " ëª¨í˜¸í•œ í‘œí˜„ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤."

            # ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ê°œì„ 
            if "ë„ˆë¬´ ì§§ì€ ì‘ë‹µ" in user_feedback:
                revised_prompt += "\n\nPlease provide detailed, comprehensive responses with clear explanations."
                changes_made.append("ë” êµ¬ì²´ì ì¸ ì‘ë‹µ ì œê³µ")
                feedback_addressed.append("ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ê°œì„ ")
                improvement_explanation += " ë” êµ¬ì²´ì ì¸ ì‘ë‹µì„ ì œê³µí–ˆìŠµë‹ˆë‹¤."

            # ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ ê°•ì¡°
            if "ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­" in user_feedback:
                revised_prompt += "\n\nPlease prioritize important instructions and ensure they are clear."
                changes_made.append("ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•œ ìš°ì„ ìˆœìœ„ ëª…í™•íˆ í•˜ê¸°")
                feedback_addressed.append("ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ ê°•ì¡°")
                improvement_explanation += " ì¤‘ìš”í•œ ì§€ì‹œì‚¬í•­ì— ëŒ€í•œ ìš°ì„ ìˆœìœ„ë¥¼ ëª…í™•íˆ í–ˆìŠµë‹ˆë‹¤."

            # ë„êµ¬ ì‚¬ìš© ì§€ì¹¨ ì¶”ê°€
            if "ë„êµ¬ ì‚¬ìš©" in user_feedback:
                revised_prompt += "\n\nIf you are not sure about information needed for the task, use available tools to gather relevant information - do NOT guess or make up an answer."
                changes_made.append("ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ ì¶”ê°€")
                feedback_addressed.append("ë„êµ¬ ì‚¬ìš© ì§€ì¹¨ ì¶”ê°€")
                improvement_explanation += " ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ëª…í™•í•œ ì§€ì¹¨ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."

            # ê³„íš ìˆ˜ë¦½ ì§€ì¹¨ ì¶”ê°€
            if "ê³„íš ìˆ˜ë¦½" in user_feedback:
                revised_prompt += "\n\nPlan extensively before taking action, and reflect on the outcomes of your actions."
                changes_made.append("ê³„íš ìˆ˜ë¦½ ì§€ì¹¨ ì¶”ê°€")
                feedback_addressed.append("ê³„íš ìˆ˜ë¦½ ì§€ì¹¨ ì¶”ê°€")
                improvement_explanation += " ê³„íš ìˆ˜ë¦½ ë° ë°˜ì„±ì  ì‚¬ê³ ì— ëŒ€í•œ ì§€ì¹¨ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."

            # ì¶œë ¥ í˜•ì‹ ëª…ì‹œ
            if 'format' not in revised_prompt.lower():
                revised_prompt += "\n\nProvide your response in a clear, structured format."
                changes_made.append("ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ ì¶”ê°€")
                feedback_addressed.append("ì¶œë ¥ í˜•ì‹ ëª…ì‹œ")
                improvement_explanation += " ì¶œë ¥ í˜•ì‹ì„ ëª…ì‹œí–ˆìŠµë‹ˆë‹¤."

            result = RevisedPrompt(
                original_optimized_prompt=original_optimized_prompt,
                user_feedback=user_feedback,
                revised_prompt=revised_prompt,
                changes_made=changes_made,
                feedback_addressed=feedback_addressed,
                improvement_explanation=improvement_explanation
            )
            
            if progress_callback:
                progress_callback(f"âœ… í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì™„ë£Œ: {len(changes_made)}ê°œ ê°œì„ ì‚¬í•­ ì ìš©")
            
            return Result(result)
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return Result(RevisedPrompt(
                original_optimized_prompt=input_data,
                user_feedback=input_data,
                revised_prompt=input_data,
                changes_made=[],
                feedback_addressed=[],
                improvement_explanation="í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            ))

    @staticmethod
    async def _general_response(agent: Agent, input_data: str, progress_callback=None):
        """ë²”ìš© agent ì‹œë®¬ë ˆì´ì…˜"""
        if progress_callback:
            progress_callback("ğŸ¤– ë²”ìš© ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
        
        class Result:
            def __init__(self, final_output):
                self.final_output = final_output
        
        try:
            data = json.loads(input_data)
            original_prompt = data.get("original_prompt", "")
            user_feedback = data.get("user_feedback", "")
            
            modified_prompt = original_prompt
            changes_made = []
            explanation = "ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤."
            
            # ì–¸ì–´ ë³€ê²½ ì²˜ë¦¬
            if "í•œê¸€" in user_feedback or "korean" in user_feedback.lower():
                if "translate" in user_feedback.lower() or "ë³€ê²½" in user_feedback or "ë²ˆì—­" in user_feedback:
                    # ì˜ì–´ -> í•œê¸€ ë²ˆì—­ ì‹œë®¬ë ˆì´ì…˜
                    if "You are" in original_prompt:
                        modified_prompt = original_prompt.replace("You are", "ë‹¹ì‹ ì€")
                        modified_prompt = modified_prompt.replace("Please", "ë‹¤ìŒì„")
                        modified_prompt = modified_prompt.replace("Provide", "ì œê³µí•˜ì„¸ìš”")
                        modified_prompt = modified_prompt.replace("Create", "ìƒì„±í•˜ì„¸ìš”") 
                        modified_prompt = modified_prompt.replace("Analyze", "ë¶„ì„í•˜ì„¸ìš”")
                        modified_prompt = modified_prompt.replace("Write", "ì‘ì„±í•˜ì„¸ìš”")
                        changes_made.append("í”„ë¡¬í”„íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­")
                        explanation = "í”„ë¡¬í”„íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤."
            
            # ì˜ì–´ ë³€ê²½ ì²˜ë¦¬
            elif "english" in user_feedback.lower() or "ì˜ì–´" in user_feedback:
                if "translate" in user_feedback.lower() or "ë³€ê²½" in user_feedback or "ë²ˆì—­" in user_feedback:
                    # í•œê¸€ -> ì˜ì–´ ë²ˆì—­ ì‹œë®¬ë ˆì´ì…˜
                    if "ë‹¹ì‹ ì€" in original_prompt:
                        modified_prompt = original_prompt.replace("ë‹¹ì‹ ì€", "You are")
                        modified_prompt = modified_prompt.replace("ë‹¤ìŒì„", "Please")
                        modified_prompt = modified_prompt.replace("ì œê³µí•˜ì„¸ìš”", "provide")
                        modified_prompt = modified_prompt.replace("ìƒì„±í•˜ì„¸ìš”", "create")
                        modified_prompt = modified_prompt.replace("ë¶„ì„í•˜ì„¸ìš”", "analyze")
                        modified_prompt = modified_prompt.replace("ì‘ì„±í•˜ì„¸ìš”", "write")
                        changes_made.append("í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­")
                        explanation = "í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤."
            
            # ë¬¸ì²´ ë³€ê²½ ì²˜ë¦¬
            elif "formal" in user_feedback.lower() or "ì •ì¤‘" in user_feedback or "ê²©ì‹" in user_feedback:
                modified_prompt = f"Please {modified_prompt.lstrip('Please ')}"
                if not modified_prompt.endswith('.'):
                    modified_prompt += "."
                changes_made.append("ì •ì¤‘í•œ ë¬¸ì²´ë¡œ ë³€ê²½")
                explanation = "í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì¤‘í•œ ë¬¸ì²´ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤."
            
            elif "casual" in user_feedback.lower() or "ì¹œê·¼" in user_feedback or "í¸í•œ" in user_feedback:
                modified_prompt = modified_prompt.replace("Please ", "")
                modified_prompt = modified_prompt.replace("Kindly ", "")
                changes_made.append("ì¹œê·¼í•œ ë¬¸ì²´ë¡œ ë³€ê²½")
                explanation = "í”„ë¡¬í”„íŠ¸ë¥¼ ì¹œê·¼í•œ ë¬¸ì²´ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤."
            
            # ê¸¸ì´ ì¡°ì •
            elif "shorter" in user_feedback.lower() or "ì§§ê²Œ" in user_feedback or "ê°„ë‹¨" in user_feedback:
                sentences = modified_prompt.split('.')
                modified_prompt = '. '.join(sentences[:len(sentences)//2]) + '.'
                changes_made.append("í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë‹¨ì¶•")
                explanation = "í”„ë¡¬í”„íŠ¸ë¥¼ ë” ê°„ê²°í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤."
            
            elif "longer" in user_feedback.lower() or "ê¸¸ê²Œ" in user_feedback or "ìì„¸íˆ" in user_feedback:
                modified_prompt += " Please provide detailed explanations and comprehensive responses."
                changes_made.append("í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¥")
                explanation = "í”„ë¡¬í”„íŠ¸ì— ë” ìì„¸í•œ ì„¤ëª…ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤."
            
            # ê¸°ë³¸ ì²˜ë¦¬
            if not changes_made:
                modified_prompt += f"\n\n[User Request: {user_feedback}]"
                changes_made.append("ì‚¬ìš©ì ìš”ì²­ ì‚¬í•­ ì¶”ê°€")
                explanation = f"ì‚¬ìš©ì ìš”ì²­ '{user_feedback}'ì„ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤."
            
            result = GeneralResponse(
                original_prompt=original_prompt,
                modified_prompt=modified_prompt,
                changes_made=changes_made,
                explanation=explanation,
                feedback_addressed=user_feedback
            )
            
            if progress_callback:
                progress_callback(f"âœ… ë²”ìš© ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: {len(changes_made)}ê°œ ë³€ê²½ì‚¬í•­ ì ìš©")
            
            return Result(result)
            
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ ë²”ìš© ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return Result(GeneralResponse(
                original_prompt=input_data,
                modified_prompt=input_data,
                changes_made=[],
                explanation="ë²”ìš© ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                feedback_addressed=input_data
            ))

def preserve_markdown_emphasis(original: str, optimized: str, issues: list) -> str:
    """ì›ë³¸ì˜ ë§ˆí¬ë‹¤ìš´ ê°•ì¡°(*, **)ë¥¼ ìµœì í™” í”„ë¡¬í”„íŠ¸ì— ë³µì› (í° ì˜¤ë¥˜ê°€ ì—†ì„ ë•Œë§Œ)"""
    # checkerì—ì„œ ê°•ì¡° ë¶€ë¶„ì´ ë¬¸ì œë¼ê³  ì§€ì í•œ ê²½ìš°ëŠ” ë³µì›í•˜ì§€ ì•ŠìŒ
    issue_text = ' '.join(issues).lower() if issues else ''
    if any(x in issue_text for x in ['emphasis', 'bold', 'italic', 'star', 'asterisk', 'markdown', 'formatting']):
        return optimized
    
    # ë§ˆí¬ë‹¤ìš´ ê°•ì¡° íŒ¨í„´ ì¶”ì¶œ
    patterns = [r'\*\*[^*]+\*\*', r'\*[^*]+\*', r'__[^_]+__', r'_[^_]+_']
    for pat in patterns:
        for match in re.findall(pat, original):
            # ê°•ì¡°ëœ í…ìŠ¤íŠ¸ì—ì„œ * ë˜ëŠ” _ ì œê±°
            plain = match.strip('*_')
            # ìµœì í™” í”„ë¡¬í”„íŠ¸ì— ê°•ì¡°ê°€ ì‚¬ë¼ì¡Œë‹¤ë©´ ë³µì›
            if plain in optimized and match not in optimized:
                optimized = optimized.replace(plain, match)
    return optimized

def create_agents(model: str = "gpt-4o"):
    """ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ Agent ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ìƒì„±"""
    return {
        # OpenAI Cookbook ì›ë³¸ checkerë“¤
        "dev_contradiction_checker": Agent(
            name="contradiction_detector",
            model=model,
            output_type=Issues,
            instructions="""
You are **Dev-Contradiction-Checker**.

Goal
Detect *genuine* self-contradictions or impossibilities **inside** the developer prompt supplied in the variable `DEVELOPER_MESSAGE`.

Definitions
â€¢ A contradiction = two clauses that cannot both be followed.
â€¢ Overlaps or redundancies in the DEVELOPER_MESSAGE are *not* contradictions.

What you MUST do
1. Compare every imperative / prohibition against all others.
2. List at most FIVE contradictions (each as ONE bullet).
3. If no contradiction exists, say so.

Output format (**strict JSON**)
Return **only** an object that matches the `Issues` schema:

```json
{"has_issues": <bool>,
"issues": [
    "<bullet 1>",
    "<bullet 2>"
]
}
- has_issues = true IFF the issues array is non-empty.
- Do not add extra keys, comments or markdown.
"""
        ),
        "format_checker": Agent(
            name="format_checker",
            model=model,
            output_type=Issues,
            instructions="""
You are Format-Checker.

Task
Decide whether the developer prompt requires a structured output (JSON/CSV/XML/Markdown table, etc.).
If so, flag any missing or unclear aspects of that format.

Steps
Categorise the task as:
a. "conversation_only", or
b. "structured_output_required".

For case (b):
- Point out absent fields, ambiguous data types, unspecified ordering, or missing error-handling.

Do NOT invent issues if unsure. be a little bit more conservative in flagging format issues

Output format
Return strictly-valid JSON following the Issues schema:

{
"has_issues": <bool>,
"issues": ["<desc 1>", "..."]
}
Maximum five issues. No extra keys or text.
"""
        ),
        # ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ë“¤
        "prompt_type_detector": Agent(
            name="prompt_type_detector",
            model=model,
            output_type=PromptTypeDetection,
            instructions="""
You are Prompt-Type-Detector.
Your task is to analyze the given prompt and determine its type and characteristics.

Analyze the prompt for:
1. Primary purpose (creative_writing, code_generation, qa, analysis, instruction_following, etc.)
2. Key characteristics that define this type
3. Appropriate optimization strategy for this type
4. Which checkers would be most relevant

Be specific and confident in your assessment.
"""
        ),
        "prompt_candidate_generator": Agent(
            name="prompt_candidate_generator",
            model=model,
            output_type=PromptCandidateOutput,
            instructions="""
You are Prompt-Candidate-Generator.
You receive:
- BASE_PROMPT_IDEA (a string with the core user request)
- NUM_CANDIDATES (an integer for how many variations to create)

Your task is to generate diverse variations of the BASE_PROMPT_IDEA.

Generation rules:
- Rephrase the core request using different wording.
- Change the prompt structure (e.g., from question to instruction).
- Add or modify constraints and personas.
- Preserve the original intent of the base idea.

Output format (strict JSON)
{
  "prompt_candidates": ["<full text of candidate 1>", "<full text of candidate 2>"]
}
No other keys, no markdown.
"""
        ),
        "performance_ranker_selector": Agent(
            name="performance_ranker_selector",
            model=model,
            output_type=PerformanceRankingOutput,
            instructions="""
You are Performance-Ranker-and-Selector.
You receive:
- CANDIDATE_EVALUATIONS: A list of objects, where each object contains a prompt candidate and its evaluation scores.
  Example item: {"prompt": "...", "format_score": 1.0, "contradiction_score": 0.9, "relevance_score": 0.7}

Your task is to aggregate the scores for each candidate and rank them to find the best-performing prompt.

Ranking rules:
- Calculate a weighted average score for each candidate based on its evaluation results.
- Rank the prompts from highest to lowest score.

Output format (strict JSON)
{
  "ranked_prompts": [
    {"prompt": "<full text of best prompt>", "final_score": <float>, "rank": 1},
    {"prompt": "<full text of second-best prompt>", "final_score": <float>, "rank": 2}
  ]
}
No other keys, no markdown.
"""
        ),
        "relevance_goal_alignment_evaluator": Agent(
            name="relevance_goal_alignment_evaluator",
            model=model,
            output_type=RelevanceEvaluationOutput,
            instructions="""
You are Relevance-and-Goal-Alignment-Evaluator.
You receive:
- ORIGINAL_PROMPT (the prompt given to the model)
- MODEL_OUTPUT (the text generated by the model)

Your task is to evaluate how well the MODEL_OUTPUT satisfies the user's intent expressed in the ORIGINAL_PROMPT.

Evaluation criteria:
- Relevance: Is the output on-topic?
- Completeness: Does the output fully answer the user's request?
- Accuracy: Does the output provide a correct and useful response to the core task?

Output format (strict JSON)
{
  "alignment_score": <float from 0.0 to 1.0>,
  "evaluation_summary": "<brief explanation of the score, noting any gaps in relevance or completeness>"
}
No other keys, no markdown.
"""
        ),
        "safety_bias_checker": Agent(
            name="safety_bias_checker",
            model=model,
            output_type=SafetyCheckOutput,
            instructions="""
You are Safety-and-Bias-Checker.
You receive:
- TEXT_TO_CHECK (can be a prompt or a model output)

Your task is to scan the text for harmful content, social biases, toxic language, and Personally Identifiable Information (PII).

Detection rules:
- Flag any content that falls into predefined safety categories.
- Identify subtle social biases and stereotypes.
- Detect potential PII like names, emails, or phone numbers.

Output format (strict JSON)
{
  "is_safe": <boolean>,
  "safety_flags": [
    {"category": "<e.g., HATE_SPEECH>", "details": "<specific text flagged>"}
  ]
}
No other keys, no markdown.
"""
        ),
        "prompt_optimizer": Agent(
            name="prompt_optimizer",
            model=model,
            output_type=OptimizedPrompt,
            instructions="Optimize prompts based on GPT-4.1 best practices"
        ),
        "feedback_analyzer": Agent(
            name="feedback_analyzer",
            model=model,
            output_type=FeedbackAnalysis,
            instructions="Analyze user feedback to understand their concerns and suggest improvements"
        ),
        "prompt_reviser": Agent(
            name="prompt_reviser",
            model=model,
            output_type=RevisedPrompt,
            instructions="Revise the prompt based on user feedback to improve clarity and adherence"
        )
    }

# ë©”ì¸ ìµœì í™” í•¨ìˆ˜
async def optimize_prompt_comprehensive(
    prompt: str,
    prompt_type: str = None,
    num_candidates: int = 3,
    progress_callback=None,
    api_key: str = None,
    model: str = "gpt-4o"
) -> Dict[str, Any]:
    """GPT-4.1 ê°€ì´ë“œë¼ì¸ ê¸°ë°˜ ì¢…í•©ì  í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
    
    if progress_callback:
        progress_callback("ğŸš€ ì¢…í•©ì  í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì‹œì‘...")
    
    # Agent ìƒì„±
    agents = create_agents(model)
    
    # 0ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìœ í˜• ê°ì§€ (ìœ í˜•ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
    detected_type = prompt_type
    type_detection_result = None
    
    if not prompt_type:
        if progress_callback:
            progress_callback("ğŸ” í”„ë¡¬í”„íŠ¸ ìœ í˜• ìë™ ê°ì§€ ì¤‘...")
        
        type_detection_result = await Runner.run(
            agents["prompt_type_detector"], 
            prompt, 
            progress_callback, 
            api_key
        )
        detected_type = type_detection_result.final_output.detected_type
        
        if progress_callback:
            progress_callback(f"âœ… í”„ë¡¬í”„íŠ¸ ìœ í˜• ê°ì§€: {detected_type} (ì‹ ë¢°ë„: {type_detection_result.final_output.confidence:.2f})")
    
    # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„±
    if progress_callback:
        progress_callback(f"ğŸ”„ {num_candidates}ê°œì˜ í”„ë¡¬í”„íŠ¸ ë³€í˜• ìƒì„± ì¤‘...")
    
    candidate_input = {
        "BASE_PROMPT_IDEA": prompt,
        "NUM_CANDIDATES": num_candidates
    }
    
    candidates_result = await Runner.run(
        agents["prompt_candidate_generator"],
        json.dumps(candidate_input),
        progress_callback,
        api_key
    )
    
    prompt_candidates = candidates_result.final_output.prompt_candidates
    
    # 2ë‹¨ê³„: ê° í›„ë³´ì— ëŒ€í•œ ë³‘ë ¬ ë¶„ì„
    if progress_callback:
        progress_callback("ğŸ“Š ê° í”„ë¡¬í”„íŠ¸ í›„ë³´ ë¶„ì„ ì¤‘...")
    
    all_candidate_evaluations = []
    
    for candidate in prompt_candidates:
        # ê° í›„ë³´ì— ëŒ€í•œ ê¸°ë³¸ ì²´í¬
        analysis_tasks = [
            Runner.run(agents["dev_contradiction_checker"], f"DEVELOPER_MESSAGE: {candidate}", progress_callback, api_key),
            Runner.run(agents["format_checker"], candidate, progress_callback, api_key),
            Runner.run(agents["safety_bias_checker"], json.dumps({"TEXT_TO_CHECK": candidate}), progress_callback, api_key),
        ]
        
        analysis_results = await asyncio.gather(*analysis_tasks)
        
        # ì ìˆ˜ ê³„ì‚°
        contradiction_score = 1.0 if not analysis_results[0].final_output.has_issues else 0.5
        format_score = 1.0 if not analysis_results[1].final_output.has_issues else 0.7
        safety_result = analysis_results[2].final_output
        safety_score = 1.0 if safety_result.is_safe else 0.3
        
        all_candidate_evaluations.append({
            "prompt": candidate,
            "contradiction_score": contradiction_score,
            "format_score": format_score,
            "safety_score": safety_score,
            "relevance_score": 0.8  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶œë ¥ê³¼ ë¹„êµí•´ì•¼ í•¨
        })
    
    # 3ë‹¨ê³„: ì„±ëŠ¥ ìˆœìœ„ ë§¤ê¸°ê¸°
    if progress_callback:
        progress_callback("ğŸ† ìµœì  í”„ë¡¬í”„íŠ¸ ì„ íƒ ì¤‘...")
    
    ranking_input = {
        "CANDIDATE_EVALUATIONS": all_candidate_evaluations
    }
    
    ranking_result = await Runner.run(
        agents["performance_ranker_selector"],
        json.dumps(ranking_input),
        progress_callback,
        api_key
    )
    
    # 4ë‹¨ê³„: ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì„ íƒ ë° ì¶”ê°€ ê°œì„ 
    best_prompt = ranking_result.final_output.ranked_prompts[0]["prompt"]
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimization_input = {
        "original_prompt": prompt,
        "all_issues": [],
        "detected_type": detected_type,
        "best_candidate": best_prompt
    }
    
    optimization_result = await Runner.run(
        agents["prompt_optimizer"], 
        json.dumps(optimization_input),
        progress_callback,
        api_key
    )
    
    return {
        "original_prompt": prompt,
        "detected_type": detected_type,
        "type_detection": type_detection_result.final_output.model_dump() if type_detection_result else None,
        "prompt_candidates": prompt_candidates,
        "candidate_evaluations": all_candidate_evaluations,
        "ranking_results": ranking_result.final_output.model_dump(),
        "optimized_prompt": optimization_result.final_output.optimized_prompt,
        "optimization_details": optimization_result.final_output.model_dump(),
        "estimated_improvement": optimization_result.final_output.estimated_improvement
    }

async def revise_prompt_with_feedback(
    optimized_prompt: str,
    user_feedback: str,
    progress_callback=None,
    api_key: str = None,
    model: str = "gpt-4o"
) -> Dict[str, Any]:
    """í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€ ê°œì„ """
    
    if progress_callback:
        progress_callback("ğŸ” í”¼ë“œë°± ë¶„ì„ ì‹œì‘...")
    
    # 1ë‹¨ê³„: í”¼ë“œë°± ë¶„ì„
    feedback_input = {
        "user_feedback": user_feedback
    }
    
    # Agent ìƒì„±
    agents = create_agents(model)
    
    feedback_analysis_result = await Runner.run(
        agents["feedback_analyzer"],
        json.dumps(feedback_input),
        progress_callback,
        api_key
    )
    
    if progress_callback:
        progress_callback("âœï¸ í”¼ë“œë°± ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì‹œì‘...")
    
    # 2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
    revision_input = {
        "original_optimized_prompt": optimized_prompt,
        "user_feedback": user_feedback
    }
    
    revision_result = await Runner.run(
        agents["prompt_reviser"],
        json.dumps(revision_input),
        progress_callback,
        api_key
    )
    
    return {
        "original_optimized_prompt": optimized_prompt,
        "user_feedback": user_feedback,
        "feedback_analysis": feedback_analysis_result.final_output.model_dump(),
        "revision_details": revision_result.final_output.model_dump(),
        "revised_prompt": revision_result.final_output.revised_prompt,
        "changes_made": revision_result.final_output.changes_made,
        "feedback_addressed": revision_result.final_output.feedback_addressed
    }

# ë²”ìš© Agent ì²˜ë¦¬ í•¨ìˆ˜
async def run_general_agent(original_prompt: str, user_feedback: str, api_key: str = None, model: str = "gpt-4o"):
    """ë²”ìš© agentë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬"""
    
    def progress_callback(message):
        if hasattr(st.session_state, 'progress_messages'):
            timestamp = asyncio.get_event_loop().time()
            formatted_time = f"{int(timestamp % 86400 // 3600):02d}:{int(timestamp % 3600 // 60):02d}:{int(timestamp % 60):02d}"
            st.session_state.progress_messages.append(f"{formatted_time} {message}")

    # ë²”ìš© Agent ìƒì„±
    general_agent = Agent(
        name="general_agent",
        model=model,
        output_type=GeneralResponse,
        instructions=f"""
        You are a versatile prompt modification agent. Your task is to modify the given prompt according to the user's specific feedback and requirements.
        
        Capabilities:
        - Language translation (Korean â†” English, etc.)
        - Style adjustment (formal â†” casual, technical â†” simple)
        - Content modification (add/remove specific elements)
        - Format changes (structured â†” narrative)
        - Tone adjustment (professional, friendly, authoritative)
        - Length adjustment (expand or compress)
        - Any other reasonable prompt modifications
        
        Always provide:
        1. The original prompt
        2. The modified prompt according to user feedback
        3. Specific changes made
        4. Clear explanation of modifications
        5. How the user feedback was addressed
        """
    )
    
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    input_data = {
        "original_prompt": original_prompt,
        "user_feedback": user_feedback,
        "instruction": "Modify the prompt according to the user feedback. Be precise and thorough."
    }
    
    # Agent ì‹¤í–‰
    result = await Runner.run(
        general_agent,
        json.dumps(input_data),
        progress_callback,
        api_key
    )
    
    if result and hasattr(result, 'final_output'):
        return {
            "original_prompt": original_prompt,
            "user_feedback": user_feedback,
            "modified_prompt": result.final_output.modified_prompt,
            "changes_made": result.final_output.changes_made,
            "explanation": result.final_output.explanation,
            "feedback_addressed": result.final_output.feedback_addressed
        }
    
    return None